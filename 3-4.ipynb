{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble learners - bagging and boosting\n",
    "\n",
    "In 1988, Michael Kearns and Leslie Valiant posed the following question: \"Can a set of weak learners be combined to create a single strong learner?\"\n",
    "\n",
    "- The answer came back in 2009 with Netflix 1 million prize competition; the wining team called themselves \"The Ensemble\"\n",
    "- https://en.wikipedia.org/wiki/Netflix_Prize#2009\n",
    "\n",
    "## Ensemble \n",
    "- not one but combination of several\n",
    "- dictionary defination: a group of items viewed as a whole rather then individually"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble learners\n",
    "\n",
    "- creating an ensemble of learners is one way to make the learners you got better\n",
    "- It is not about creating new algorithm buts instead it is about assembling together different algorithms or model to create an ensemble learners\n",
    "- This idea can be used with any model or method you know. for eg combine KNN & regerssion; it's an ensemble. \n",
    "- All algorithm will be trained using same data\n",
    "- To query, the ensemble of learners query each model and with same x and each of them have some y output which we now need to combine\n",
    "- How to combine y from each model?\n",
    "  - for classification: might have each of them vote and pick the result of highest vote! Democracy!\n",
    "  - for regression: take the mean of y\n",
    "- Then test the overall ensemble learner using the test data set aside.\n",
    "\n",
    "![Ensemble learner - see image 3-4_1_ensemble.png](images/3_4_1_ensemble.png)\n",
    "\n",
    "## Why ensembles?\n",
    "- lower error\n",
    "- less overfitting\n",
    "  - why? \n",
    "    - Each learner has it's own bias; but when you put them together, it tends to reduce the biases because they are fighting against each other in some sort of way\n",
    "- tastes great"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quiz: How to build an ensemble?\n",
    "\n",
    "- Use KNN & regression learner, how can you build an ensemble? Consider the approach ABCD and check the one that you think is best\n",
    "\n",
    "![Ensemble learner quiz - see image 3-4_2_ensemble_quiz.png ](images/3-4_2_ensemble_quiz.png)\n",
    "\n",
    "\n",
    "- C & E are a terrible idea\n",
    "- Next step: let's look at how do B"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bootstrap aggregating - bagging\n",
    "\n",
    "- `Bagging`: Model built using same learning algorithm but train each learner on different set of data.\n",
    "- How bagging works:\n",
    "  - create number of subsets of data -> D1, D2, ... Dm by selecting data randomly with replacement\n",
    "  - Because of replacement, same data points might be repeated in a bag or across the bag and that's ok\n",
    "  - we created m bags and each bag has same number of data points n<sup>'</sup>\n",
    "  - we always want n<sup>'</sup> < n and usually by 60%\n",
    "  - now use each of these bags/collection of data to train a model; we now have m different models\n",
    "  - just like ensemble of different learning algorithms, we have ensemble of models.\n",
    "  - Query: for each model we query for same input and collect the output; finally we take the mean of output and that is the final y for the ensemble\n",
    "\n",
    "![Bagging - see image 3-4_3_bagging.png ](images/3-4_3_bagging.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overfitting: Quiz\n",
    "\n",
    "Which of these is most likely to overfit?\n",
    "\n",
    "![Overfitting - see image 3-4_4_overfitting_quiz.png ](images/3-4_4_overfitting_quiz.png)\n",
    "\n",
    "We saw in lecture [3-3](./3-3.ipynb), a 1NN model (kNN with k = 1) matches the training data exactly, thus overfitting.\n",
    "\n",
    "An ensemble of such learners trained on slightly different datasets will at least be able to provide some generalization, and typically less out-of-sample error. Let's see why next\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bagging Example\n",
    "\n",
    "- Randomly select some points and call this our first bag - model 1 (1NN)\n",
    "- Lets select few other data points and call this our second bag - model 2 (1NN)\n",
    "- Each of these models are clearly overfitting\n",
    "- Now let's consider an ensemble of model built using these 2 models - blue color\n",
    "\n",
    "![Bagging Eg 1 - see image 3-4_5_bagging_Eg_1.png ](images/3-4_5_bagging_Eg_1.png)\n",
    "    \n",
    "- Now lets add some more 1NN model and all of them clearly overfits\n",
    "- Here is what the ensemble will look like over this model:\n",
    "\n",
    "![Bagging Eg 2 - see image 3-4_5_bagging_Eg_2.png ](images/3-4_5_bagging_Eg_2.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Boosting: Ada Boost\n",
    "\n",
    "- Fairly simple variation on bagging that strives to improve the learner by focusing on areas where the system is not performing well\n",
    "- One of the most well known algorithms for this is `Ada Boost` (`Ada` stands for `Adaptive`)\n",
    "- How it works:\n",
    "  - Build first bag in usual way & train the model in usual way (as described in bagging method above)\n",
    "  - Next thing we do is different: take all our training data and use it to test the model. Then we will discover that for some the x's for which y's are not well predicted\n",
    "  - Now when we go to build our next bag of data, again, we choose randomly from original data; but each instance is weighted according to the error from above step. The points with more error are more likely to get picked and go into this bag then other individual instance (in picture for the D2 bag, non green dots are the error from D1 and are now picked in bag D2; also notice we got other data points too beside these error ones)\n",
    "  - Now build model from this data and test our system again - i.e. test using both the learner by using our sample data, and combine their outputs (a miniature ensemble system)\n",
    "\n",
    "  ![Boosting 1 - see image 3-4_5_boosting_1.png ](images/3-4_5_boosting_1.png)\n",
    "\n",
    "  - Now finally, measure the error of this ensemble result across all the training data. Maybe this time our original error got modeled better but have other points that were not modeled better. With this, now we build 3<sup>rd</sup> bag. \n",
    "    \n",
    "  ![Boosting 2 - see image 3-4_5_boosting_2.png ](images/3-4_5_boosting_2.png)\n",
    "\n",
    "  - We continue this over and over again until we get to the total numbers of bag we wanted to build\n",
    "\n",
    "\n",
    "`Boosting summary`: In short boosting is an add-on idea where in subsequent bags we choose the data instances that have been modeled poorly in the overall system before.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quiz: Overfitation\n",
    "\n",
    "Which is more likely to overfit as m increase?\n",
    "<br/>\n",
    "- Simple Bagging\n",
    "- Ada Boost\n",
    "\n",
    "`Answer:` Ada boost \n",
    "\n",
    "`Explanation:` As m increases, AdaBoost tries to assign more and more specific data points to subsequent learners, trying to model all the difficult examples.\n",
    "\n",
    "Thus, compared to simple bagging, it may result in more overfitting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary of Boosting and bagging\n",
    "\n",
    "- wrappers for existing methods:\n",
    "  - Boosting and Bagging are meta algorithms that takes your existing learners and convert them into an ensemble\n",
    "- reduces error\n",
    "- reduces overfitage"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}