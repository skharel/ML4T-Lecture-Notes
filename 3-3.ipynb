{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assessing a learning algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What happens to K as we vary it for KNN?\n",
    "\n",
    "- For K = N, the resulting value will be average value for every single query. This is just a constant line.\n",
    "- For K =1, this model will pick every data point that is a neighbor; in other words, it works too hard to fit every data point.\n",
    "\n",
    "### In KNN, as we increase K are we more likely to over fit?\n",
    "- In case of K = 1, we overfit; for K = N we have a constant for all query. So, we do not overfit with higher values of K\n",
    "\n",
    "### Example:\n",
    "![KNN Eg](./images/3-3_1_KNN.png)\n",
    "\n",
    "\n",
    "- Notice that beyond the boundary of data, the line is constant. We cannot extrapolate with KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Considering a parametric model (polynomial model)\n",
    "\n",
    "## Suppose we are using a polynomial of degree d.\n",
    "\n",
    "- Match Q1 with model on side\n",
    "- Answer Q2\n",
    "\n",
    "![Parametric Eg](./images/3-3_2_Parameteric.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1:\n",
    "- For d = 1, it is a linear model (equation of line such as y = mx + c). and it matches with graph c\n",
    "- For d = 2, it a quadratic equation of the form y = ax<sup>2</sup> + bx + c. Graph a matches this\n",
    "- For d = 3, it has to be b\n",
    "\n",
    "### Q2:\n",
    "- In above graph, we see that going from 1 to 3, we are tagging more and more data. Therefore, as the order of polynomial increases we are more likely to tag all data points and therefore over fit\n",
    "\n",
    "### Beyond boundaries (or edges of data):\n",
    "- Using the parametric model, we can extrapolate in the direction data seems to be going which we couldn't do in KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metric 1: RMS Error\n",
    "\n",
    "- Standard way to measure error\n",
    "<br/> <br />\n",
    "![RMS-1](./images/3-3_3_RMS_1.png)\n",
    "<br/> <br />  \n",
    "- Suppose we use the training data (green dots in the plot) to build the model, say a linear model. We can assess the model at each real data point and measure the difference between y value of data point and the model; this difference is the error. We have got an error at every single data points which looks like the image below\n",
    "<br/> <br />\n",
    "![RMS-2](./images/3-3_3_RMS_2.png)\n",
    "<br/> <br />\n",
    "\n",
    "- To Measure RMS Error, take error at each data points: square every error, sum them and average them. Finally take square root of it! \n",
    "- This is an approximation of average errors but we end up emphasizing larger errors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## In Sample vs Out of Sample\n",
    "\n",
    "- For the sample data (training data) we can build a model that can fit the training  set exactly\n",
    "- But the more important measure is \"What is our error out of sample?\". \n",
    "  - Out of sample means, we train on training set; but for testing we test on separate set of data - testing set\n",
    "  - To measure our out of sample error we measure on testing set (not on the training set)\n",
    "- In the graph below, look at the blue data points and plug it into our RMSE Equation \n",
    "<br/><br/>\n",
    "\n",
    "![OOS](./images/3-3_4_Out_of_Sample.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quiz?\n",
    "\n",
    "Q: Which error would you expect to be larger?\n",
    "<br/>\n",
    "[A] in sample error (training set)\n",
    "<br/>\n",
    "[B] out of sample error (test set)\n",
    "\n",
    "\n",
    "=> B"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation\n",
    "\n",
    "- When evaluating a learning algorithm, they split their data into 2 chunks - training (60% of data set) & testing (40 % of data set)\n",
    "- You train on a data and then you test on it, this is one trial. In many cases that's enough; you measure your RMS and that's the assessment of your algorithm and compare it against another algorithm\n",
    "<br/><br/>\n",
    "![CV](./images/3-3_5-CV_1.png)\n",
    "\n",
    "<br/>\n",
    "- However, sometimes the `problem encountered is that researchers do not have enough data` to analyze their algorithm. In such case, they can create more data by slicing it up and running more trails.\n",
    "- One approach can be that slice data into 5 chunks and use 80% of it for training and use the 20% of it for training. Suppose we used first 4 chunks for training and last 1 for testing; this one one trial.\n",
    "- Then we can switch things up: say we reserve first data set for testing and last four for training; this will be another trial\n",
    "- We can again switch things up and so on. With this slicing into 5 chunks, we can get 5 different trials out of this one data set.\n",
    "<br/><br/>\n",
    "\n",
    "![CV](./images/3-3_5-CV_2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Roll forward cross validation:\n",
    "\n",
    "- Cross validation is great tool but it doesn't fit well for financial data application because it can permit peeking into the future. For instance, if our training slice is after the test slice we are peeking into future ahead of our test which can result in unrealistically optimistic results.\n",
    "- With this sort of financial data we need to avoid peeking; one way to avoid it is with roll forward cross validation - means training data is always before test data. Even in this case we can have multiple trials by rolling our data forward until we run out of data\n",
    "\n",
    "<img alt=\"3-3_5-RF_1.png\" src=\"images/3-3_5-RF_1.png\" width=\"800\">\n",
    "\n",
    "\n",
    "<img alt=\"3-3_5-RF_2.png\" src=\"images/3-3_5-RF_2.png\" height=\"300\" width=\"150\">\n",
    "<img alt=\"3-3_5-RF_3.png\" src=\"images/3-3_5-RF_3.png\" height=\"300\" width=\"150\">\n",
    "<img alt=\"3-3_5-RF_4.png\" src=\"images/3-3_5-RF_4.png\" height=\"300\" width=\"150\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metric 2: correlation\n",
    "\n",
    "- another way to visualize and evaluate the accuracy of regression algorithm is to look at the relationship between predicted and the actual value of our dependent variable y\n",
    "- Y<sub>predict</sub> = result from our model for given X<sub>test</sub>\n",
    "- real value of y is Y<sub>test</sub>\n",
    "- Now plot a scatter plot Y<sub>predict</sub> and Y<sub>test</sub> & fit a line\n",
    "- Correlation: Are the data points close to the lines?\n",
    "- We can measure this property of quantitatively using co-relation. \n",
    "- `Numpy` provides a function `np.corrcoef` to measure the value which ranges from -1 to +1. Values towards +1 means positively co-related; -1 means inversely co-related; Values towards 0 means there is no corelation\n",
    "\n",
    "![corr1](./images/3-3_6-corr_1.png)\n",
    "\n",
    "Correlation is not a slope. See this awesome 2 part video if you have no idea what correlation means: <br/>\n",
    "- https://www.youtube.com/watch?v=qtaqvPAeEJY\n",
    "- https://www.youtube.com/watch?v=xZ_z8KWkhXE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question: Correlation and RMSError\n",
    "\n",
    "![Quiz](./images/3-3_7_corr_rmse_quiz.png)\n",
    "\n",
    "-> In general as RMS Error increase means predicted value is off from the real value; hence, correlation decreases\n",
    "\n",
    "(But it is also possible to construct eg where as RMSE increases, corr might increase; so Option 3 is also ok to pick)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overfitting\n",
    "\n",
    "- let's create a multiple polynomial model, where we increase degree 1 at a time; so we start from d = 1 and go to 2, 3, 4 and so on\n",
    "- Lets graph this where x is degree of polynomial (or degree of freedom) and on y graph the error of our model. \n",
    "- Graph for in sample error and out of sample error: <br/>  <br/>\n",
    "![OF](./images/3-3_8-overfitting.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "- For in sample error: as our degree increases, our error drops\n",
    "- For out sample error: as our degree increases, our error drops; but eventually we will reach  point where our out of sample error will start to increase again (may increase strongly). \n",
    "- The region where in sample error is decreasing but our out of sample error is increasing is the region where we start `overfitting`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN Overfitting\n",
    "\n",
    "![KNN Overfitting Quiz](./images/3-3_9_quiz.png)\n",
    "\n",
    "### My thoughts\n",
    "IN sample:\n",
    "- For K = 1, it fits every dat point so error is low\n",
    "- For K = N, its constant and misses by a lot! so error is high\n",
    "- starts low and ends high\n",
    "\n",
    "Out of sample: hmm...\n",
    "- For K = 1, it can best describe the training data set perfectly and therefore low hope for out of sample; so error must be high\n",
    "- For K = N, it is still constant and must miss a ton\n",
    "- Starts high, ends high\n",
    "\n",
    "=> b\n",
    "\n",
    "### Given explanation\n",
    "When k = 1, the model fits the training data perfectly, therefore in-sample error is low (ideally, zero). Out-of-sample error can be quite high.\n",
    "\n",
    "As k increases, the model becomes more generalized, thus out-of-sample error decreases at the cost of slightly increasing in-sample error.\n",
    "\n",
    "After a certain point, the model becomes too general and starts performing worse on both training and test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Few other considerations for evaluating learning algorithm\n",
    "\n",
    "Which algorithm is better - Linear Regression or KNN?\n",
    "\n",
    "![qz](./images/3-3_10_quiz.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('ml4t': conda)"
  },
  "interpreter": {
   "hash": "977b4f815527edff52d42f7c09580f6f48eb00923934407c2fb133389eb94d28"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}